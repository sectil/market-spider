#!/usr/bin/env python3
"""
Trendyol Review Scraper - ÃœrÃ¼n yorumlarÄ±nÄ± Ã§eker
"""

import re
import json
import time
import cloudscraper
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from typing import List, Dict
from database import SessionLocal, ProductReview, Product
from turkish_review_ai import TurkishReviewAI

class TrendyolReviewScraper:
    """Trendyol yorum scraper"""

    def __init__(self):
        self.scraper = cloudscraper.create_scraper()
        self.ai = TurkishReviewAI()
        self.session = SessionLocal()

    def get_product_reviews(self, product_url: str, max_pages: int = 5) -> List[Dict]:
        """ÃœrÃ¼n yorumlarÄ±nÄ± Ã§ek"""
        reviews = []

        # Product ID'yi URL'den Ã§Ä±kar
        product_id_match = re.search(r'-p-(\d+)', product_url)
        if not product_id_match:
            print(f"âŒ Product ID bulunamadÄ±: {product_url}")
            return reviews

        product_id = product_id_match.group(1)
        print(f"ğŸ“¦ Product ID: {product_id}")

        # YorumlarÄ± Ã§ek (sayfalama ile)
        for page in range(1, max_pages + 1):
            page_reviews = self._get_reviews_page(product_id, page)
            if not page_reviews:
                break

            reviews.extend(page_reviews)
            print(f"  âœ“ Sayfa {page}: {len(page_reviews)} yorum bulundu")

            # Rate limiting
            time.sleep(1)

        print(f"âœ… Toplam {len(reviews)} yorum Ã§ekildi")
        return reviews

    def _get_reviews_page(self, product_id: str, page: int) -> List[Dict]:
        """Belirli bir sayfa yorumlarÄ± Ã§ek"""
        reviews = []

        # Trendyol API endpoint - GÃ¼ncel URL
        url = f"https://api.trendyol.com/webproductgw/api/review/{product_id}"

        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'application/json',
            'Accept-Language': 'tr-TR,tr;q=0.9,en;q=0.8',
            'Referer': 'https://www.trendyol.com/'
        }

        params = {
            'page': page - 1,  # 0-indexed
            'size': 20,
            'sortBy': 'helpfulCount'  # En faydalÄ± yorumlar
        }

        try:
            response = self.scraper.get(url, params=params, headers=headers)
            if response.status_code != 200:
                # Alternatif yÃ¶ntem: Direkt Ã¼rÃ¼n sayfasÄ±ndan Ã§ek
                return self._get_reviews_from_product_page(product_id, page)

            data = response.json()

            if 'result' in data and 'productReviews' in data['result']:
                review_data = data['result']['productReviews']

                for review_content in review_data.get('content', []):
                    # Yorum verilerini parse et
                    review = {
                        'reviewer_name': review_content.get('userFullName', 'Anonim'),
                        'reviewer_verified': review_content.get('verifiedPurchase', False),
                        'rating': review_content.get('rate', 0),
                        'review_title': review_content.get('reviewTitle', ''),
                        'review_text': review_content.get('comment', ''),
                        'review_date': self._parse_date(review_content.get('commentDateISOtype')),
                        'helpful_count': review_content.get('helpfulCount', 0),
                        'seller_name': review_content.get('sellerName', '')
                    }

                    # BoÅŸ yorumlarÄ± atla
                    if review['review_text']:
                        reviews.append(review)

        except Exception as e:
            print(f"âŒ API hatasÄ±: {e}")
            # Alternatif yÃ¶ntem dene
            return self._get_reviews_from_product_page(product_id, page)

        return reviews

    def _get_reviews_from_product_page(self, product_id: str, page: int) -> List[Dict]:
        """ÃœrÃ¼n sayfasÄ±ndan direkt yorum Ã§ek - Alternatif yÃ¶ntem"""
        reviews = []

        try:
            # ÃœrÃ¼n sayfasÄ±nÄ± bul
            product = self.session.query(Product).filter(
                Product.site_product_id == product_id
            ).first()

            if not product or not product.product_url:
                return reviews

            # ÃœrÃ¼n sayfasÄ±nÄ± Ã§ek
            response = self.scraper.get(product.product_url)
            if response.status_code != 200:
                return reviews

            soup = BeautifulSoup(response.text, 'html.parser')

            # JavaScript iÃ§indeki yorumlarÄ± bul
            for script in soup.find_all('script'):
                if script.string and 'window.__PRODUCT_DETAIL_APP_INITIAL_STATE__' in script.string:
                    match = re.search(r'window\.__PRODUCT_DETAIL_APP_INITIAL_STATE__\s*=\s*({.*?});',
                                    script.string, re.DOTALL)
                    if match:
                        data = json.loads(match.group(1))

                        # YorumlarÄ± Ã§Ä±kar
                        if 'product' in data and 'reviews' in data['product']:
                            for review in data['product']['reviews']['content']:
                                reviews.append({
                                    'reviewer_name': review.get('userFullName', 'Anonim'),
                                    'reviewer_verified': review.get('verifiedPurchase', False),
                                    'rating': review.get('rate', 0),
                                    'review_title': review.get('reviewTitle', ''),
                                    'review_text': review.get('comment', ''),
                                    'review_date': self._parse_date(review.get('lastModifiedDate')),
                                    'helpful_count': review.get('helpfulCount', 0),
                                    'seller_name': review.get('sellerName', '')
                                })

        except Exception as e:
            print(f"âš ï¸ Sayfa parse hatasÄ±: {e}")

        return reviews

    def _parse_date(self, date_str: str) -> datetime:
        """Tarih string'ini parse et"""
        if not date_str:
            return datetime.now()

        try:
            # ISO format: "2024-03-15T10:30:00.000Z"
            return datetime.fromisoformat(date_str.replace('Z', '+00:00'))
        except:
            return datetime.now()

    def scrape_and_analyze_product_reviews(self, product_id: int):
        """VeritabanÄ±ndaki bir Ã¼rÃ¼n iÃ§in yorumlarÄ± Ã§ek ve analiz et"""

        # ÃœrÃ¼nÃ¼ bul
        product = self.session.query(Product).filter_by(id=product_id).first()
        if not product:
            print(f"âŒ ÃœrÃ¼n bulunamadÄ±: {product_id}")
            return

        print(f"\nğŸ” Yorumlar Ã§ekiliyor: {product.name[:50]}...")

        # YorumlarÄ± Ã§ek
        reviews = self.get_product_reviews(product.product_url or product.url, max_pages=10)

        if not reviews:
            print("âŒ Yorum bulunamadÄ±")
            return

        print(f"ğŸ“ {len(reviews)} yorum analiz ediliyor...")

        # Her yorumu analiz et ve kaydet
        for review_data in reviews:
            # AI analizi
            analysis = self.ai.analyze_review(review_data['review_text'])

            # VeritabanÄ±na kaydet
            review = ProductReview(
                product_id=product_id,
                reviewer_name=review_data['reviewer_name'],
                reviewer_verified=review_data['reviewer_verified'],
                rating=review_data['rating'],
                review_title=review_data['review_title'],
                review_text=review_data['review_text'],
                review_date=review_data['review_date'],
                helpful_count=review_data['helpful_count'],

                # AI analiz sonuÃ§larÄ±
                sentiment_score=analysis['sentiment_score'],
                key_phrases=analysis['key_phrases'],
                purchase_reasons=analysis['purchase_reasons'],
                pros=analysis['pros'],
                cons=analysis['cons']
            )

            self.session.add(review)

        self.session.commit()
        print(f"âœ… {len(reviews)} yorum kaydedildi ve analiz edildi")

        # Toplu analiz
        reviews_for_bulk = [{'text': r['review_text'], 'rating': r['rating'],
                            'verified': r['reviewer_verified'], 'helpful_count': r['helpful_count']}
                           for r in reviews]

        bulk_analysis = self.ai.analyze_bulk_reviews(reviews_for_bulk)

        # Analiz sonuÃ§larÄ±nÄ± gÃ¶ster
        self._display_analysis_results(product.name, bulk_analysis)

        return bulk_analysis

    def _display_analysis_results(self, product_name: str, analysis: Dict):
        """Analiz sonuÃ§larÄ±nÄ± gÃ¶rselleÅŸtir"""

        print("\n" + "="*60)
        print(f"ğŸ“Š YORUM ANALÄ°Z RAPORU: {product_name[:40]}...")
        print("="*60)

        # Genel istatistikler
        print(f"\nğŸ“ˆ GENEL Ä°STATÄ°STÄ°KLER:")
        print(f"  â€¢ Toplam Yorum: {analysis['total_reviews']}")
        print(f"  â€¢ Ortalama Duygu: {analysis['average_sentiment']:.2f} (-1 ile +1 arasÄ±)")
        print(f"  â€¢ Tavsiye Skoru: {analysis['recommendation_score']:.1f}/100")
        print(f"  â€¢ DoÄŸrulanmÄ±ÅŸ AlÄ±cÄ±: %{analysis['verified_percentage']:.1f}")

        # Duygu daÄŸÄ±lÄ±mÄ±
        print(f"\nğŸ˜Š DUYGU DAÄILIMI:")
        dist = analysis['sentiment_distribution']
        print(f"  â€¢ Pozitif: %{dist['pozitif']:.1f} ğŸ˜Š")
        print(f"  â€¢ NÃ¶tr: %{dist['nÃ¶tr']:.1f} ğŸ˜")
        print(f"  â€¢ Negatif: %{dist['negatif']:.1f} ğŸ˜")

        # SatÄ±n alma nedenleri
        print(f"\nğŸ›’ SATIN ALMA NEDENLERÄ°:")
        for reason, count in analysis['top_purchase_reasons']:
            print(f"  â€¢ {reason.title()}: {count} kiÅŸi")

        # ArtÄ±lar
        print(f"\nâœ… EN Ã‡OK BELÄ°RTÄ°LEN ARTILAR:")
        for pro, count in analysis['top_pros'][:5]:
            print(f"  â€¢ {pro}: {count} kez bahsedildi")

        # Eksiler
        if analysis['top_cons']:
            print(f"\nâŒ EN Ã‡OK BELÄ°RTÄ°LEN EKSÄ°LER:")
            for con, count in analysis['top_cons'][:5]:
                print(f"  â€¢ {con}: {count} kez bahsedildi")

        # MÃ¼ÅŸteri davranÄ±ÅŸ tipleri
        print(f"\nğŸ‘¥ MÃœÅTERÄ° DAVRANIÅ TÄ°PLERÄ°:")
        for behavior, count in analysis['behavior_distribution'].items():
            print(f"  â€¢ {behavior.replace('_', ' ').title()}: {count} kiÅŸi")

        # Ä°nsan davranÄ±ÅŸÄ± iÃ§gÃ¶rÃ¼leri
        insights = analysis['human_insights']
        print(f"\nğŸ§  Ä°NSAN DAVRANIÅI ANALÄ°ZÄ°:")
        print(f"\nğŸ“ Ana Motivasyon:")
        print(f"  {insights['ana_motivasyon']}")

        print(f"\nğŸ‘¤ MÃ¼ÅŸteri Profili:")
        print(f"  {insights['mÃ¼ÅŸteri_profili']}")

        print(f"\nğŸ¯ SatÄ±n Alma Psikolojisi:")
        print(f"  {insights['satÄ±n_alma_psikolojisi']}")

        print(f"\nâœ¨ BaÅŸarÄ± FaktÃ¶rleri:")
        for factor in insights['baÅŸarÄ±_faktÃ¶rleri']:
            print(f"  â€¢ {factor}")

        if insights['risk_faktÃ¶rleri'] and insights['risk_faktÃ¶rleri'][0] != 'Belirgin risk faktÃ¶rÃ¼ yok':
            print(f"\nâš ï¸ Risk FaktÃ¶rleri:")
            for risk in insights['risk_faktÃ¶rleri']:
                print(f"  â€¢ {risk}")

        print(f"\nğŸ“Œ GENEL TAVSÄ°YE:")
        print(f"  {insights['tavsiye']}")

        print("\n" + "="*60)

    def scrape_all_best_sellers(self):
        """En Ã§ok satanlarÄ±n hepsinin yorumlarÄ±nÄ± Ã§ek"""
        # Son 7 gÃ¼nÃ¼n en Ã§ok satanlarÄ±nÄ± bul
        from sqlalchemy import func
        from database import RankingHistory

        seven_days_ago = datetime.now() - timedelta(days=7)

        best_sellers = self.session.query(
            Product
        ).join(
            RankingHistory
        ).filter(
            RankingHistory.timestamp >= seven_days_ago,
            RankingHistory.rank_position <= 10  # Top 10
        ).distinct().all()

        print(f"ğŸ“¦ {len(best_sellers)} en Ã§ok satan Ã¼rÃ¼n bulundu")

        for product in best_sellers:
            print(f"\n{'='*60}")
            print(f"Ä°ÅŸleniyor: {product.name[:50]}...")

            # Daha Ã¶nce yorum Ã§ekilmiÅŸ mi kontrol et
            existing_reviews = self.session.query(ProductReview).filter_by(
                product_id=product.id
            ).count()

            if existing_reviews > 0:
                print(f"  â„¹ï¸ {existing_reviews} yorum zaten mevcut, atlanÄ±yor...")
                continue

            # YorumlarÄ± Ã§ek ve analiz et
            self.scrape_and_analyze_product_reviews(product.id)

            # Rate limiting
            time.sleep(2)

        print("\nâœ… TÃ¼m en Ã§ok satanlarÄ±n yorumlarÄ± analiz edildi")


if __name__ == "__main__":
    scraper = TrendyolReviewScraper()

    # Test: Ä°lk Ã¼rÃ¼nÃ¼n yorumlarÄ±nÄ± Ã§ek
    session = SessionLocal()
    first_product = session.query(Product).first()

    if first_product:
        print(f"Test Ã¼rÃ¼n: {first_product.name}")
        scraper.scrape_and_analyze_product_reviews(first_product.id)
    else:
        print("âŒ Test edilecek Ã¼rÃ¼n bulunamadÄ±")

    session.close()