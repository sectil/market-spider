#!/usr/bin/env python3
"""
ULTRA Otomatik Trendyol Yorum Scraper
Chrome olmadan Ã§alÄ±ÅŸÄ±r - Sadece requests ve akÄ±llÄ± API kullanÄ±mÄ±
FALLBACK YOK! %100 GERÃ‡EK VERÄ°!
"""

import requests
import json
import time
import random
import re
from datetime import datetime
from database import SessionLocal, Product, ProductReview
from turkish_review_ai import TurkishReviewAI
import cloudscraper

class UltraAutoScraper:
    """Ultra otomatik scraper - Chrome gerekmez, API odaklÄ±"""

    def __init__(self):
        self.session = SessionLocal()
        self.ai = TurkishReviewAI()
        self.scraper = cloudscraper.create_scraper()

    def extract_product_id(self, url):
        """URL'den product ID Ã§Ä±kar"""
        patterns = [
            r'-p-(\d+)',
            r'/product/(\d+)',
            r'productId=(\d+)',
            r'/(\d+)\?',
            r'/(\d+)$'
        ]

        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                return match.group(1)

        return None

    def get_product_data_from_page(self, product_url):
        """ÃœrÃ¼n sayfasÄ±ndan veri Ã§ek"""
        print("ğŸ“„ ÃœrÃ¼n sayfasÄ± analiz ediliyor...")

        try:
            response = self.scraper.get(product_url, timeout=15)

            if response.status_code != 200:
                return None

            html = response.text

            # JavaScript iÃ§indeki JSON verisini bul
            pattern = r'window\.__PRODUCT_DETAIL_APP_INITIAL_STATE__\s*=\s*({.*?});'
            match = re.search(pattern, html, re.DOTALL)

            if match:
                try:
                    data = json.loads(match.group(1))
                    return data
                except:
                    pass

            # Alternatif pattern
            pattern2 = r'<script>window\.TYPageName="product_detail";window\.\_\_data\_\_=({.*?})</script>'
            match2 = re.search(pattern2, html)

            if match2:
                try:
                    data = json.loads(match2.group(1))
                    return data
                except:
                    pass

        except Exception as e:
            print(f"  âŒ Sayfa Ã§ekme hatasÄ±: {e}")

        return None

    def extract_reviews_from_page_data(self, page_data):
        """Sayfa verisinden yorumlarÄ± Ã§Ä±kar"""
        reviews = []

        if not page_data:
            return reviews

        # FarklÄ± yollardan yorumlarÄ± bul
        paths = [
            lambda d: d.get('product', {}).get('ratingSummary', {}).get('reviews', []),
            lambda d: d.get('product', {}).get('reviews', {}).get('content', []),
            lambda d: d.get('product', {}).get('reviews', []),
            lambda d: d.get('reviews', []),
            lambda d: d.get('ratingSummary', {}).get('reviews', [])
        ]

        for path_func in paths:
            try:
                review_list = path_func(page_data)
                if review_list and isinstance(review_list, list):
                    for r in review_list:
                        if r.get('comment') or r.get('text'):
                            reviews.append({
                                'reviewer_name': r.get('userFullName', '') or r.get('userName', '') or 'Trendyol MÃ¼ÅŸterisi',
                                'reviewer_verified': r.get('verifiedPurchase', False),
                                'rating': r.get('rate', 5) or r.get('rating', 5),
                                'review_text': r.get('comment', '') or r.get('text', ''),
                                'review_date': self._parse_date(r.get('commentDateISOtype') or r.get('date')),
                                'helpful_count': r.get('helpfulCount', 0)
                            })
                    if reviews:
                        print(f"  âœ“ Sayfadan {len(reviews)} yorum bulundu")
                        return reviews
            except:
                continue

        return reviews

    def get_reviews_via_api(self, product_id):
        """API Ã¼zerinden yorumlarÄ± Ã§ek"""
        reviews = []

        # Ã‡eÅŸitli API endpoint'leri
        endpoints = [
            f"https://public.trendyol.com/discovery-web-webproductgw-santral/api/review/{product_id}",
            f"https://public.trendyol.com/discovery-web-webproductgw-santral/review/{product_id}",
            f"https://public.trendyol.com/discovery-web-productgw-service/api/review/{product_id}",
            f"https://public-mdc.trendyol.com/discovery-web-productgw-service/reviews/{product_id}",
            f"https://api.trendyol.com/webproductgw/api/review/{product_id}",
            f"https://public.trendyol.com/reviews-service/api/product/{product_id}/reviews"
        ]

        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'tr-TR,tr;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Origin': 'https://www.trendyol.com',
            'Referer': 'https://www.trendyol.com/',
            'Sec-Ch-Ua': '"Not_A Brand";v="8", "Chromium";v="120"',
            'Sec-Ch-Ua-Mobile': '?0',
            'Sec-Ch-Ua-Platform': '"Windows"',
            'Sec-Fetch-Dest': 'empty',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Site': 'same-origin'
        }

        for endpoint in endpoints:
            print(f"  ğŸ“¡ API deneniyor: {endpoint[:60]}...")

            try:
                # Her endpoint iÃ§in farklÄ± sayfalama dene
                for page in range(5):
                    params = {
                        'page': page,
                        'size': 50,
                        'sortBy': 'helpfulCount',
                        'culture': 'tr-TR',
                        'storefrontId': '1',
                        'culturetype': '0'
                    }

                    response = self.scraper.get(
                        endpoint,
                        headers=headers,
                        params=params,
                        timeout=10
                    )

                    if response.status_code == 200:
                        try:
                            data = response.json()

                            # YorumlarÄ± bul
                            review_list = self._extract_reviews_from_api_response(data)

                            if review_list:
                                reviews.extend(review_list)
                                print(f"    âœ“ Sayfa {page+1}: {len(review_list)} yorum")

                                if len(review_list) < 20:
                                    break  # Son sayfa

                                time.sleep(random.uniform(0.5, 1))
                            else:
                                break

                        except Exception as e:
                            break

                    elif response.status_code == 404:
                        break

                    else:
                        break

                if reviews:
                    print(f"  âœ… Bu API'den toplam {len(reviews)} yorum alÄ±ndÄ±")
                    return reviews

            except Exception as e:
                continue

        return reviews

    def _extract_reviews_from_api_response(self, data):
        """API response'dan yorumlarÄ± Ã§Ä±kar"""
        reviews = []

        # FarklÄ± JSON formatlarÄ±nÄ± dene
        paths = [
            lambda d: d.get('result', {}).get('productReviews', {}).get('content', []),
            lambda d: d.get('result', {}).get('content', []),
            lambda d: d.get('content', []),
            lambda d: d.get('reviews', {}).get('content', []),
            lambda d: d.get('reviews', []),
            lambda d: d.get('productReviews', {}).get('content', []),
            lambda d: d if isinstance(d, list) else []
        ]

        for path_func in paths:
            try:
                review_list = path_func(data)
                if review_list and isinstance(review_list, list):
                    for r in review_list:
                        comment = r.get('comment', '') or r.get('text', '') or r.get('reviewText', '')
                        if comment and len(comment) > 5:
                            reviews.append({
                                'reviewer_name': r.get('userFullName', '') or r.get('userName', '') or 'Trendyol MÃ¼ÅŸterisi',
                                'reviewer_verified': r.get('verifiedPurchase', False) or r.get('isVerified', False),
                                'rating': r.get('rate', 5) or r.get('rating', 5) or r.get('score', 5),
                                'review_text': comment,
                                'review_date': self._parse_date(r.get('commentDateISOtype') or r.get('date') or r.get('createdDate')),
                                'helpful_count': r.get('helpfulCount', 0) or r.get('helpfulVotes', 0)
                            })
                    if reviews:
                        return reviews
            except:
                continue

        return reviews

    def _parse_date(self, date_str):
        """Tarih parse et"""
        if not date_str:
            return datetime.now()

        try:
            # ISO format
            if 'T' in str(date_str):
                return datetime.fromisoformat(str(date_str).replace('Z', '+00:00'))

            # Unix timestamp
            if isinstance(date_str, (int, float)) or (isinstance(date_str, str) and date_str.isdigit()):
                timestamp = int(date_str)
                if timestamp > 1000000000000:  # Milliseconds
                    timestamp = timestamp / 1000
                return datetime.fromtimestamp(timestamp)

        except:
            pass

        return datetime.now()

    def ultra_auto_scrape(self, product_id):
        """Ultra otomatik scraping - FALLBACK YOK!"""

        # ÃœrÃ¼nÃ¼ bul
        product = self.session.query(Product).filter_by(id=product_id).first()
        if not product:
            print(f"âŒ ÃœrÃ¼n bulunamadÄ±: {product_id}")
            return False

        print("\n" + "="*60)
        print("ğŸš€ ULTRA OTOMATÄ°K YORUM Ã‡EKME")
        print("="*60)
        print(f"ğŸ“¦ ÃœrÃ¼n: {product.name[:50]}...")
        print(f"ğŸ”— URL: {product.product_url or product.url}")
        print("âš ï¸ FALLBACK YOK - %100 GERÃ‡EK VERÄ°!")
        print("-"*60)

        # Mevcut yorumlarÄ± temizle
        self.session.query(ProductReview).filter_by(product_id=product_id).delete()
        self.session.commit()

        all_reviews = []

        # 1. Ã–nce sayfadan veri Ã§ek
        page_data = self.get_product_data_from_page(product.product_url or product.url)
        if page_data:
            page_reviews = self.extract_reviews_from_page_data(page_data)
            if page_reviews:
                all_reviews.extend(page_reviews)
                print(f"âœ… Sayfadan {len(page_reviews)} yorum alÄ±ndÄ±")

        # 2. Product ID Ã§Ä±kar
        product_trendyol_id = self.extract_product_id(product.product_url or product.url)

        if not product_trendyol_id and product.site_product_id:
            product_trendyol_id = product.site_product_id

        # 3. API'den yorumlarÄ± Ã§ek
        if product_trendyol_id:
            print(f"\nğŸ“¡ API aramasÄ± (Product ID: {product_trendyol_id})...")
            api_reviews = self.get_reviews_via_api(product_trendyol_id)
            if api_reviews:
                all_reviews.extend(api_reviews)
                print(f"âœ… API'den {len(api_reviews)} yorum alÄ±ndÄ±")

        # Duplicate'leri kaldÄ±r
        unique_reviews = []
        seen_texts = set()
        for review in all_reviews:
            if review['review_text'] not in seen_texts:
                unique_reviews.append(review)
                seen_texts.add(review['review_text'])

        if not unique_reviews:
            print("\n" + "="*60)
            print("âŒ GERÃ‡EK YORUM Ã‡EKÄ°LEMEDÄ°!")
            print("âŒ FALLBACK KULLANILMADI!")
            print("âš ï¸ Trendyol API'sine ulaÅŸÄ±lamadÄ±")
            print("="*60)
            return False

        # YorumlarÄ± kaydet
        print(f"\nğŸ’¾ {len(unique_reviews)} GERÃ‡EK yorum kaydediliyor...")

        for review_data in unique_reviews[:100]:  # Max 100 yorum
            # AI analizi
            analysis = self.ai.analyze_review(review_data['review_text'])

            # VeritabanÄ±na kaydet
            review = ProductReview(
                product_id=product_id,
                reviewer_name=review_data['reviewer_name'],
                reviewer_verified=review_data['reviewer_verified'],
                rating=review_data['rating'],
                review_title='',
                review_text=review_data['review_text'],
                review_date=review_data['review_date'],
                helpful_count=review_data['helpful_count'],
                sentiment_score=analysis['sentiment_score'],
                key_phrases=analysis['key_phrases'],
                purchase_reasons=analysis['purchase_reasons'],
                pros=analysis['pros'],
                cons=analysis['cons']
            )

            self.session.add(review)

        self.session.commit()

        # Analiz gÃ¶ster
        self._show_real_analysis(product.name, unique_reviews[:100])

        print("\n" + "="*60)
        print(f"âœ… {len(unique_reviews[:100])} GERÃ‡EK YORUM KAYDEDÄ°LDÄ°!")
        print("âœ… %100 GERÃ‡EK VERÄ° - FALLBACK YOK!")
        print("="*60)

        return True

    def _show_real_analysis(self, product_name, reviews):
        """GERÃ‡EK yorum analizini gÃ¶ster"""

        # AI ile toplu analiz
        reviews_for_ai = [
            {
                'text': r['review_text'],
                'rating': r['rating'],
                'verified': r['reviewer_verified'],
                'helpful_count': r['helpful_count']
            }
            for r in reviews
        ]

        analysis = self.ai.analyze_bulk_reviews(reviews_for_ai)

        print("\n" + "="*60)
        print(f"ğŸ“Š {product_name[:40]}... GERÃ‡EK YORUM ANALÄ°ZÄ°")
        print("="*60)

        print(f"\nğŸ“ˆ GERÃ‡EK Ä°STATÄ°STÄ°KLER:")
        print(f"  â€¢ Toplam GERÃ‡EK Yorum: {analysis['total_reviews']}")
        print(f"  â€¢ Ortalama Duygu: {analysis['average_sentiment']:.2f}")
        print(f"  â€¢ Tavsiye Skoru: {analysis['recommendation_score']:.1f}/100")

        if 'verified_percentage' in analysis:
            print(f"  â€¢ DoÄŸrulanmÄ±ÅŸ AlÄ±cÄ±: %{analysis['verified_percentage']:.1f}")

        print(f"\nğŸ›’ NEDEN 1. SIRADA? (GERÃ‡EK MÃœÅTERÄ° GÃ–RÃœÅLERÄ°):")
        for reason, count in analysis['top_purchase_reasons'][:5]:
            print(f"  â€¢ {reason}: {count} kiÅŸi")

        print(f"\nâœ… GERÃ‡EK ARTILAR:")
        for pro, count in analysis['top_pros'][:5]:
            print(f"  â€¢ {pro}: {count} kez belirtilmiÅŸ")

        if analysis['top_cons']:
            print(f"\nâŒ GERÃ‡EK EKSÄ°LER:")
            for con, count in analysis['top_cons'][:3]:
                print(f"  â€¢ {con}: {count} kez belirtilmiÅŸ")

        insights = analysis['human_insights']
        print(f"\nğŸ§  GERÃ‡EK MÃœÅTERÄ° DAVRANIÅI ANALÄ°ZÄ°:")
        print(f"  {insights['ana_motivasyon']}")

        if 'satÄ±n_alma_psikolojisi' in insights:
            print(f"\nğŸ¯ NEDEN BU ÃœRÃœN 1. SIRADA:")
            print(f"  {insights['satÄ±n_alma_psikolojisi']}")

        print("\n" + "="*60)


if __name__ == "__main__":
    print("="*60)
    print("âš¡ ULTRA OTOMATÄ°K SCRAPER")
    print("âœ… Chrome gerekmez")
    print("âœ… AkÄ±llÄ± API kullanÄ±mÄ±")
    print("âŒ FALLBACK YOK!")
    print("="*60)

    scraper = UltraAutoScraper()

    # Ä°lk Ã¼rÃ¼nÃ¼ test et
    first_product = scraper.session.query(Product).first()
    if first_product:
        scraper.ultra_auto_scrape(first_product.id)
    else:
        print("âŒ ÃœrÃ¼n bulunamadÄ±")

    scraper.session.close()