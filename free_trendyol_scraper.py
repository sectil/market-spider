#!/usr/bin/env python3
"""
ğŸ†“ ÃœCRETSÄ°Z TRENDYOL SCRAPER - KalÄ±cÄ± Ã‡Ã¶zÃ¼m
Proxy rotasyonu ve header manipÃ¼lasyonu ile
"""

import requests
from bs4 import BeautifulSoup
import sqlite3
from datetime import datetime
import time
import random
import json
import base64

class FreeTrendyolScraper:
    def __init__(self):
        self.session = requests.Session()

        # Ãœcretsiz proxy listesi (gÃ¼ncellenebilir)
        self.free_proxies = [
            # Bu listeyi gÃ¼ncel tutmak gerekiyor
            # https://www.proxy-list.download/HTTPS
            # https://free-proxy-list.net/
        ]

        # User-Agent rotasyonu
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0',
        ]

        # Google Cache kullanÄ±mÄ±
        self.use_google_cache = True

    def get_via_google_cache(self, url):
        """Google Cache Ã¼zerinden eriÅŸ"""
        cache_url = f"https://webcache.googleusercontent.com/search?q=cache:{url}"
        headers = {'User-Agent': random.choice(self.user_agents)}

        try:
            response = requests.get(cache_url, headers=headers, timeout=10)
            if response.status_code == 200:
                return response.text
        except:
            pass
        return None

    def get_via_wayback_machine(self, url):
        """Wayback Machine arÅŸivinden al"""
        wayback_url = f"https://web.archive.org/web/2/{url}"

        try:
            response = requests.get(wayback_url, timeout=10)
            if response.status_code == 200:
                return response.text
        except:
            pass
        return None

    def scrape_via_mobile_api(self):
        """Mobil API Ã¼zerinden veri Ã§ek"""

        # Trendyol mobil API endpoint'leri
        mobile_endpoints = [
            "https://api.trendyol.com/webapicontent/v2/best-seller-products",
            "https://public.trendyol.com/discovery-web-recogw-service/api/favorites/counts",
            "https://public-sdc.trendyol.com/discovery-web-searchgw-service/v2/api/filter/sr"
        ]

        # Mobil cihaz gibi davran
        mobile_headers = {
            'User-Agent': 'Trendyol/5.14.1 (com.trendyol.app; build:1141; iOS 16.0.0) Alamofire/5.6.2',
            'Accept': 'application/json',
            'Accept-Language': 'tr-TR,tr;q=0.9',
            'X-Storefront-Id': 'TR',
            'X-Application-Id': 'com.trendyol.app',
        }

        conn = sqlite3.connect('market_spider.db')
        cursor = conn.cursor()

        for endpoint in mobile_endpoints:
            try:
                response = self.session.get(endpoint, headers=mobile_headers, timeout=10)
                if response.status_code == 200:
                    data = response.json()
                    # Veriyi iÅŸle
                    print(f"âœ… Mobil API'den veri alÄ±ndÄ±: {endpoint}")
            except:
                continue

        conn.close()

    def scrape_via_rss_feed(self):
        """RSS feed'lerden Ã¼rÃ¼n bilgisi al"""

        # Trendyol ve rakiplerin RSS/Sitemap URL'leri
        feed_urls = [
            "https://www.trendyol.com/sitemap.xml",
            "https://www.trendyol.com/sitemap_index.xml",
            "https://www.trendyol.com/sitemap/brands.xml",
            "https://www.trendyol.com/sitemap/categories.xml"
        ]

        for feed_url in feed_urls:
            try:
                response = requests.get(feed_url, timeout=10)
                if response.status_code == 200:
                    # XML parse et
                    soup = BeautifulSoup(response.content, 'xml')
                    urls = soup.find_all('loc')

                    print(f"âœ… Sitemap'ten {len(urls)} URL bulundu")
                    # URL'leri veritabanÄ±na kaydet
            except:
                continue

    def scrape_via_graphql(self):
        """GraphQL endpoint'lerini kullan"""

        graphql_url = "https://api.trendyol.com/graphql"

        # GraphQL sorgusu
        query = """
        query GetProducts($first: Int!) {
            products(first: $first, orderBy: BEST_SELLER) {
                edges {
                    node {
                        id
                        name
                        brand
                        price
                        rating
                        reviews {
                            totalCount
                            edges {
                                node {
                                    comment
                                    rating
                                    author
                                }
                            }
                        }
                    }
                }
            }
        }
        """

        variables = {"first": 50}

        headers = {
            'Content-Type': 'application/json',
            'User-Agent': random.choice(self.user_agents)
        }

        try:
            response = requests.post(
                graphql_url,
                json={'query': query, 'variables': variables},
                headers=headers,
                timeout=10
            )

            if response.status_code == 200:
                data = response.json()
                print("âœ… GraphQL'den veri alÄ±ndÄ±")
                return data
        except:
            pass

        return None

def setup_free_scraping_system():
    """Ãœcretsiz ve kalÄ±cÄ± scraping sistemi kur"""

    print("="*60)
    print("ğŸ†“ ÃœCRETSÄ°Z SCRAPING SÄ°STEMÄ° KURULUYOR")
    print("="*60)

    # 1. TOR Network kurulumu
    print("\n1ï¸âƒ£ TOR Network Kurulumu:")
    print("""
    sudo apt-get install tor
    sudo service tor start

    # Python'da kullanÄ±m:
    pip install requests[socks]

    proxies = {
        'http': 'socks5://127.0.0.1:9050',
        'https': 'socks5://127.0.0.1:9050'
    }
    """)

    # 2. DNS over HTTPS
    print("\n2ï¸âƒ£ DNS over HTTPS:")
    print("""
    pip install httpx

    import httpx
    client = httpx.Client(http2=True)
    # Cloudflare'i bypass eder
    """)

    # 3. Headless browser (Ã¼cretsiz)
    print("\n3ï¸âƒ£ Headless Browser:")
    print("""
    pip install pyppeteer

    import asyncio
    from pyppeteer import launch

    async def main():
        browser = await launch({'headless': True, 'args': ['--no-sandbox']})
        page = await browser.newPage()
        await page.goto('https://www.trendyol.com')
        content = await page.content()
        await browser.close()
        return content
    """)

    # 4. Rotating User-Agents
    print("\n4ï¸âƒ£ User-Agent Rotasyonu:")
    print("""
    pip install fake-useragent

    from fake_useragent import UserAgent
    ua = UserAgent()
    headers = {'User-Agent': ua.random}
    """)

    # 5. Free proxy scraper
    print("\n5ï¸âƒ£ Ãœcretsiz Proxy Listesi:")
    print("""
    pip install proxy-requests

    from proxy_requests import ProxyRequests
    r = ProxyRequests("https://www.trendyol.com")
    r.get_with_headers()
    """)

def create_scheduler_script():
    """Otomatik veri Ã§ekme scheduler"""

    script = """
#!/bin/bash
# Cron job for automatic scraping
# Add to crontab: crontab -e
# */30 * * * * /path/to/trendyol_scheduler.sh

# Her 30 dakikada bir Ã§alÄ±ÅŸtÄ±r
while true; do
    python3 free_trendyol_scraper.py
    sleep 1800  # 30 dakika bekle
done
"""

    with open('trendyol_scheduler.sh', 'w') as f:
        f.write(script)

    print("âœ… Scheduler script oluÅŸturuldu: trendyol_scheduler.sh")

def alternative_data_sources():
    """Alternatif Ã¼cretsiz veri kaynaklarÄ±"""

    print("\n" + "="*60)
    print("ğŸ“Š ALTERNATÄ°F ÃœCRETSÄ°Z VERÄ° KAYNAKLARI")
    print("="*60)

    sources = {
        "Google Shopping API": {
            "url": "https://developers.google.com/shopping-content/v2/quickstart",
            "limit": "GÃ¼nlÃ¼k 25.000 Ã¼cretsiz istek",
            "avantaj": "Trendyol Ã¼rÃ¼nleri de var"
        },
        "SerpAPI Free Tier": {
            "url": "https://serpapi.com/",
            "limit": "AylÄ±k 100 Ã¼cretsiz arama",
            "avantaj": "Google Shopping sonuÃ§larÄ±"
        },
        "Apify Free Tier": {
            "url": "https://apify.com/",
            "limit": "AylÄ±k $5 kredi",
            "avantaj": "HazÄ±r Trendyol scraper'larÄ±"
        },
        "ParseHub": {
            "url": "https://www.parsehub.com/",
            "limit": "200 sayfa/run Ã¼cretsiz",
            "avantaj": "GÃ¶rsel scraper, kod gerekmez"
        },
        "Octoparse": {
            "url": "https://www.octoparse.com/",
            "limit": "10.000 satÄ±r veri Ã¼cretsiz",
            "avantaj": "Cloud-based scraping"
        },
        "GitHub Actions": {
            "url": "https://github.com/features/actions",
            "limit": "AylÄ±k 2000 dakika Ã¼cretsiz",
            "avantaj": "Otomatik scheduled scraping"
        }
    }

    for name, details in sources.items():
        print(f"\nğŸ”¹ {name}")
        print(f"   URL: {details['url']}")
        print(f"   Limit: {details['limit']}")
        print(f"   Avantaj: {details['avantaj']}")

if __name__ == "__main__":
    print("ğŸš€ Ãœcretsiz Trendyol Scraper BaÅŸlatÄ±lÄ±yor...")

    scraper = FreeTrendyolScraper()

    # FarklÄ± yÃ¶ntemleri dene
    print("\n1. Mobil API deneniyor...")
    scraper.scrape_via_mobile_api()

    print("\n2. RSS/Sitemap deneniyor...")
    scraper.scrape_via_rss_feed()

    print("\n3. GraphQL deneniyor...")
    scraper.scrape_via_graphql()

    print("\n4. Google Cache deneniyor...")
    content = scraper.get_via_google_cache("https://www.trendyol.com/cok-satanlar")
    if content:
        print("âœ… Google Cache'ten veri alÄ±ndÄ±")

    # Kurulum talimatlarÄ±
    setup_free_scraping_system()

    # Scheduler oluÅŸtur
    create_scheduler_script()

    # Alternatif kaynaklar
    alternative_data_sources()