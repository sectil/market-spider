"""
Admin Panel - Site ve URL Y√∂netimi
"""

import streamlit as st
import pandas as pd
import requests
from datetime import datetime
from urllib.parse import urlparse
from sqlalchemy.orm import Session
from database import SessionLocal, SiteConfig, SiteUrl

def get_db_session():
    return SessionLocal()
from base_scraper import BaseScraper
import json
import time

def show_admin_panel():
    """Admin paneli ana fonksiyonu"""
    st.header("üõ†Ô∏è Admin Panel - Site Y√∂netimi")

    tab1, tab2, tab3, tab4, tab5 = st.tabs(["üìù Site Ekle/D√ºzenle", "üîó URL Y√∂netimi", "üîç Otomatik Ke≈üif", "üß™ URL Test", "üìä Site Listesi"])

    with tab1:
        show_site_management()

    with tab2:
        show_url_management()

    with tab3:
        show_auto_discovery()

    with tab4:
        show_url_tester()

    with tab5:
        show_site_list()


def show_site_management():
    """Site ekleme/d√ºzenleme formu"""
    st.subheader("üìù Site Ekle/D√ºzenle")

    session = get_db_session()

    # Mevcut siteleri listele
    sites = session.query(SiteConfig).all()
    site_options = ["Yeni Site Ekle"] + [f"{s.site_name} ({s.site_key})" for s in sites]

    selected_site = st.selectbox("Site Se√ßin", site_options, key="site_manage_select")

    # Form deƒüi≈ükenleri
    if selected_site == "Yeni Site Ekle":
        site_data = {
            'site_key': '',
            'site_name': '',
            'base_url': '',
            'scraper_type': 'generic',
            'use_selenium': False,
            'rate_limit': 2.0,
            'headers': {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"},
            'proxy_url': '',
            'is_active': True
        }
    else:
        # Mevcut site verilerini y√ºkle
        site_key = selected_site.split('(')[1].split(')')[0]
        site = session.query(SiteConfig).filter_by(site_key=site_key).first()
        site_data = site.to_dict() if site else {}

    # Form
    with st.form("site_form"):
        col1, col2 = st.columns(2)

        with col1:
            site_key = st.text_input("Site Kodu (Kƒ±sa ƒ∞sim)*",
                                     value=site_data.get('site_key', ''),
                                     placeholder="trendyol, hepsiburada",
                                     help="Sistem i√ßinde kullanƒ±lacak benzersiz kod")

            site_name = st.text_input("Site Adƒ±*",
                                      value=site_data.get('site_name', ''),
                                      placeholder="Trendyol")

            base_url = st.text_input("Ana URL*",
                                     value=site_data.get('base_url', ''),
                                     placeholder="https://www.trendyol.com")

            scraper_type = st.selectbox("Scraper Tipi",
                                       options=['generic', 'trendyol', 'hepsiburada', 'n11', 'amazon'],
                                       index=['generic', 'trendyol', 'hepsiburada', 'n11', 'amazon'].index(
                                           site_data.get('scraper_type', 'generic')))

        with col2:
            use_selenium = st.checkbox("Selenium Kullan",
                                      value=site_data.get('use_selenium', False),
                                      help="JavaScript render gerektiren siteler i√ßin")

            rate_limit = st.number_input("Rate Limit (saniye)",
                                        min_value=0.5, max_value=10.0,
                                        value=float(site_data.get('rate_limit', 2.0)),
                                        step=0.5)

            proxy_url = st.text_input("Proxy URL (Opsiyonel)",
                                     value=site_data.get('proxy_url', ''),
                                     placeholder="http://proxy:port")

            is_active = st.checkbox("Aktif", value=site_data.get('is_active', True))

        # Headers
        st.subheader("Headers")
        headers_text = st.text_area("Headers (JSON formatƒ±nda)",
                                   value=json.dumps(site_data.get('headers', {}), indent=2),
                                   height=150)

        # Submit button
        submitted = st.form_submit_button("üíæ Kaydet", type="primary")

        if submitted:
            if not site_key or not site_name or not base_url:
                st.error("Zorunlu alanlarƒ± doldurun!")
            else:
                try:
                    headers = json.loads(headers_text)

                    if selected_site == "Yeni Site Ekle":
                        # Site key'in benzersiz olduƒüunu kontrol et
                        existing_site = session.query(SiteConfig).filter_by(site_key=site_key).first()
                        if existing_site:
                            st.error(f"‚ùå '{site_key}' site anahtarƒ± zaten kullanƒ±mda! L√ºtfen farklƒ± bir anahtar se√ßin.")
                            return

                        # Yeni site ekle
                        new_site = SiteConfig(
                            site_key=site_key,
                            site_name=site_name,
                            base_url=base_url,
                            scraper_type=scraper_type,
                            use_selenium=use_selenium,
                            rate_limit=rate_limit,
                            headers=headers,
                            proxy_url=proxy_url if proxy_url else None,
                            is_active=is_active
                        )
                        session.add(new_site)
                    else:
                        # Mevcut siteyi g√ºncelle
                        site = session.query(SiteConfig).filter_by(site_key=site_data['site_key']).first()
                        if site:
                            site.site_key = site_key
                            site.site_name = site_name
                            site.base_url = base_url
                            site.scraper_type = scraper_type
                            site.use_selenium = use_selenium
                            site.rate_limit = rate_limit
                            site.headers = headers
                            site.proxy_url = proxy_url if proxy_url else None
                            site.is_active = is_active
                            site.updated_at = datetime.utcnow()

                    session.commit()
                    st.success(f"‚úÖ {site_name} ba≈üarƒ±yla kaydedildi!")
                    st.rerun()

                except json.JSONDecodeError:
                    st.error("Headers JSON formatƒ± hatalƒ±!")
                except Exception as e:
                    session.rollback()
                    st.error(f"Hata: {str(e)}")


def show_url_management():
    """URL ekleme/d√ºzenleme"""
    st.subheader("üîó URL Y√∂netimi")

    session = get_db_session()

    # Site se√ßimi
    sites = session.query(SiteConfig).filter_by(is_active=True).all()
    if not sites:
        st.warning("√ñnce site ekleyin!")
        return

    site_options = {s.site_name: s.id for s in sites}
    selected_site_name = st.selectbox("Site Se√ßin", list(site_options.keys()), key="url_manage_site_select")
    selected_site_id = site_options[selected_site_name]

    # Mevcut URL'leri g√∂ster
    urls = session.query(SiteUrl).filter_by(site_id=selected_site_id).order_by(SiteUrl.priority).all()

    if urls:
        st.write("**Mevcut URL'ler:**")
        for url in urls:
            col1, col2, col3, col4 = st.columns([3, 2, 1, 1])
            with col1:
                st.text(url.url_path[:50] + "..." if len(url.url_path) > 50 else url.url_path)
            with col2:
                st.text(f"{url.url_type} - {url.category or 'Genel'}")
            with col3:
                st.text("‚úÖ Aktif" if url.is_active else "‚ùå Pasif")
            with col4:
                if st.button("üóëÔ∏è", key=f"delete_url_{url.id}"):
                    session.delete(url)
                    session.commit()
                    st.rerun()

    st.markdown("---")

    # Yeni URL ekleme formu
    st.write("**Yeni URL Ekle:**")
    with st.form("url_form"):
        col1, col2 = st.columns(2)

        with col1:
            url_type = st.selectbox("URL Tipi",
                                   options=['best_sellers', 'category', 'search', 'deals', 'new_products'])

            url_path = st.text_input("URL*",
                                    placeholder="/en-cok-satanlar veya tam URL")

            category = st.text_input("Kategori",
                                    placeholder="elektronik, giyim vs.")

            max_pages = st.number_input("Max Sayfa", min_value=1, max_value=100, value=1)

        with col2:
            description = st.text_input("A√ßƒ±klama",
                                      placeholder="En √ßok satan elektronik √ºr√ºnler")

            priority = st.number_input("√ñncelik", min_value=1, max_value=10, value=1,
                                     help="D√º≈ü√ºk sayƒ± = y√ºksek √∂ncelik")

            max_products = st.number_input("Max √úr√ºn", min_value=10, max_value=1000, value=100)

            is_active = st.checkbox("Aktif", value=True)

        # √ñzel selector'lar (opsiyonel)
        show_selectors = st.checkbox("√ñzel Selector Ekle")
        selectors = {}

        if show_selectors:
            st.write("CSS Selector'lar (Opsiyonel):")
            selectors['product_container'] = st.text_input("√úr√ºn Container",
                                                          placeholder="div.product-card")
            selectors['title'] = st.text_input("Ba≈ülƒ±k",
                                              placeholder="h3.product-title")
            selectors['price'] = st.text_input("Fiyat",
                                              placeholder="span.price")

        submitted = st.form_submit_button("‚ûï URL Ekle", type="primary")

        if submitted:
            if not url_path:
                st.error("URL zorunludur!")
            else:
                try:
                    new_url = SiteUrl(
                        site_id=selected_site_id,
                        url_type=url_type,
                        url_path=url_path,
                        category=category if category else None,
                        description=description if description else None,
                        is_active=is_active,
                        priority=priority,
                        max_pages=max_pages,
                        max_products=max_products,
                        selectors=selectors if selectors else {}
                    )
                    session.add(new_url)
                    session.commit()
                    st.success(f"‚úÖ URL ba≈üarƒ±yla eklendi!")
                    st.rerun()

                except Exception as e:
                    session.rollback()
                    st.error(f"Hata: {str(e)}")


def show_auto_discovery():
    """Otomatik kategori ke≈üfi"""
    st.subheader("üîç Otomatik Kategori Ke≈üfi")
    st.write("Site URL'si girin, sistem otomatik olarak t√ºm kategorileri ve en √ßok satanlarƒ± bulacak!")

    session = get_db_session()

    # Site se√ßimi veya URL giri≈üi
    col1, col2 = st.columns([1, 2])

    with col1:
        discovery_mode = st.radio(
            "Ke≈üif Modu",
            ["Mevcut Site", "Yeni URL"],
            key="discovery_mode"
        )

    with col2:
        if discovery_mode == "Mevcut Site":
            # Mevcut sitelerden se√ß
            sites = session.query(SiteConfig).filter_by(is_active=True).all()
            if not sites:
                st.warning("√ñnce site ekleyin!")
                return

            site_options = {s.site_name: s for s in sites}
            selected_site_name = st.selectbox(
                "Site Se√ßin",
                list(site_options.keys()),
                key="discovery_site_select"
            )
            selected_site = site_options[selected_site_name]
            discovery_url = selected_site.base_url
            site_id = selected_site.id
        else:
            # Manuel URL gir
            discovery_url = st.text_input(
                "Site URL'si",
                placeholder="https://www.example.com",
                key="discovery_url_input"
            )
            site_id = None

    # Geli≈ümi≈ü ayarlar
    with st.expander("‚öôÔ∏è Geli≈ümi≈ü Ayarlar"):
        use_selenium = st.checkbox("Selenium Kullan (JavaScript siteleri i√ßin)", value=False)
        max_categories = st.number_input("Max Kategori Sayƒ±sƒ±", min_value=5, max_value=100, value=20)
        spider_depth = st.number_input("√ñr√ºmcek Derinliƒüi", min_value=1, max_value=3, value=1,
                                      help="1: Sadece ana sayfa, 2: Ana sayfa + 1 seviye, 3: Derin tarama")
        st.session_state['spider_depth'] = spider_depth
        auto_save = st.checkbox("Otomatik Veritabanƒ±na Kaydet", value=True)

    # Ke≈üif butonu
    if st.button("üöÄ Ke≈üfi Ba≈ülat", type="primary", key="start_discovery"):
        if not discovery_url:
            st.error("L√ºtfen bir URL girin!")
            return

        with st.spinner("üîç Kategoriler ke≈üfediliyor... Bu birka√ß dakika s√ºrebilir."):
            try:
                # Site'ye √∂zel spider se√ß
                domain = urlparse(discovery_url).netloc.lower()

                if 'trendyol' in domain:
                    # Trendyol i√ßin √∂zel spider
                    from trendyol_spider import TrendyolSpider
                    spider = TrendyolSpider()
                    st.info(f"üï∑Ô∏è Trendyol √∂zel √∂r√ºmceƒüi kategorileri buluyor...")
                    result = spider.discover_all_categories()
                else:
                    # Diƒüer siteler i√ßin deep spider
                    from deep_category_spider import DeepCategorySpider
                    spider = DeepCategorySpider(max_depth=spider_depth)
                    st.info(f"üï∑Ô∏è √ñr√ºmcek {discovery_url} sitesinin T√úM kategorilerini {spider_depth} seviye derinlikte buluyor...")
                    result = spider.discover_all_categories_deep(discovery_url)

                if result['total'] > 0:
                    st.success(f"‚úÖ {result['total']} kategori bulundu!")

                    # Sonu√ßlarƒ± g√∂ster
                    st.write("### üìä Bulunan Kategoriler:")

                    # Kategorileri seviyelerine g√∂re g√∂ster
                    categories = result['categories']

                    # HTML tablo olu≈ütur - tƒ±klanabilir linklerle
                    html_content = """
                    <style>
                        .category-table {
                            width: 100%;
                            border-collapse: collapse;
                            margin: 20px 0;
                        }
                        .category-table th {
                            background-color: #f0f2f6;
                            padding: 12px;
                            text-align: left;
                            border-bottom: 2px solid #ddd;
                        }
                        .category-table td {
                            padding: 10px 12px;
                            border-bottom: 1px solid #eee;
                        }
                        .category-table tr:hover {
                            background-color: #f8f9fa;
                        }
                        .category-link {
                            color: #0066cc;
                            text-decoration: none;
                        }
                        .category-link:hover {
                            text-decoration: underline;
                            color: #0052a3;
                        }
                        .level-0 { padding-left: 12px; font-weight: bold; }
                        .level-1 { padding-left: 32px; }
                        .level-2 { padding-left: 52px; }
                        .level-3 { padding-left: 72px; }
                        .category-url {
                            font-size: 12px;
                            color: #666;
                            word-break: break-all;
                        }
                        .copy-btn {
                            background: #e0e0e0;
                            border: none;
                            padding: 2px 8px;
                            border-radius: 3px;
                            cursor: pointer;
                            font-size: 11px;
                        }
                        .copy-btn:hover {
                            background: #d0d0d0;
                        }
                    </style>
                    <table class="category-table">
                        <thead>
                            <tr>
                                <th style="width: 40px;">#</th>
                                <th>Kategori Adƒ±</th>
                                <th>URL</th>
                                <th style="width: 80px;">Seviye</th>
                            </tr>
                        </thead>
                        <tbody>
                    """

                    # Kategorileri g√∂ster
                    for i, cat in enumerate(categories[:max_categories], 1):
                        level = cat.get('level', cat.get('depth', 0))
                        name = cat.get('name', 'ƒ∞simsiz')
                        url = cat.get('url', '#')
                        best_url = cat.get('best_sellers_url', url)

                        # Seviyeye g√∂re girintili g√∂ster
                        indent_class = f"level-{level}"

                        html_content += f"""
                        <tr>
                            <td>{i}</td>
                            <td class="{indent_class}">
                                {'&nbsp;&nbsp;&nbsp;&nbsp;' * level}{'‚îî‚îÄ ' if level > 0 else ''}{name}
                            </td>
                            <td class="category-url">
                                <a href="{best_url}" target="_blank" class="category-link"
                                   title="En √ßok satanlarƒ± g√∂r√ºnt√ºle">
                                    {best_url[:80]}{'...' if len(best_url) > 80 else ''}
                                </a>
                            </td>
                            <td style="text-align: center;">{level}</td>
                        </tr>
                        """

                    html_content += """
                        </tbody>
                    </table>
                    """

                    if len(categories) > max_categories:
                        html_content += f"""
                        <div style="text-align: center; margin-top: 10px; color: #666;">
                            ... ve {len(categories) - max_categories} kategori daha
                        </div>
                        """

                    # HTML'i g√∂ster
                    st.markdown(html_content, unsafe_allow_html=True)

                    # ƒ∞statistikler
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("Toplam Kategori", result['total'])
                    with col2:
                        st.metric("Site", result.get('site', 'Bilinmiyor'))
                    with col3:
                        st.metric("Ke≈üif Tipi", "Otomatik")

                    # Veritabanƒ±na kaydet
                    if auto_save and site_id:
                        if st.button("üíæ Veritabanƒ±na Kaydet", key="save_discovered"):
                            with st.spinner("Kaydediliyor..."):
                                added = spider.save_categories(site_id, result['categories'])
                                if added > 0:
                                    st.success(f"‚úÖ {added} yeni kategori veritabanƒ±na eklendi!")
                                    st.rerun()
                                else:
                                    st.info("T√ºm kategoriler zaten mevcut.")

                        # JSON olarak indir
                        json_str = json.dumps(result, ensure_ascii=False, indent=2)
                        st.download_button(
                            label="üì• JSON Olarak ƒ∞ndir",
                            data=json_str,
                            file_name=f"{result['site']}_categories.json",
                            mime="application/json"
                        )

                    else:
                        st.warning("DataFrame bo≈ü!")

                else:
                    st.warning("üòî Kategori bulunamadƒ±. Site yapƒ±sƒ± farklƒ± olabilir.")

            except ImportError:
                st.error("‚ùå auto_category_discovery mod√ºl√º bulunamadƒ±!")
            except Exception as e:
                st.error(f"‚ùå Ke≈üif hatasƒ±: {str(e)}")

    # Mevcut kategorileri g√∂ster
    if discovery_mode == "Mevcut Site" and site_id:
        st.markdown("---")
        st.write("### üìã Mevcut Kategoriler:")

        existing_urls = session.query(SiteUrl).filter_by(
            site_id=site_id,
            url_type='best_sellers'
        ).order_by(SiteUrl.priority).all()

        if existing_urls:
            st.write(f"**{len(existing_urls)}** kategori mevcut:")
            for url in existing_urls[:10]:
                st.text(f"‚Ä¢ {url.category or 'Genel'}: {url.description or url.url_path[:50]}")

            if len(existing_urls) > 10:
                st.text(f"... ve {len(existing_urls) - 10} kategori daha")
        else:
            st.info("Bu site i√ßin hen√ºz kategori eklenmemi≈ü.")

def show_url_tester():
    """URL test aracƒ±"""
    st.subheader("üß™ URL Test")

    session = get_db_session()

    # Site ve URL se√ßimi
    sites = session.query(SiteConfig).filter_by(is_active=True).all()
    if not sites:
        st.warning("Site bulunamadƒ±!")
        return

    site_options = {s.site_name: s for s in sites}
    selected_site_name = st.selectbox("Site Se√ßin", list(site_options.keys()), key="test_site_select")
    selected_site = site_options[selected_site_name]

    urls = session.query(SiteUrl).filter_by(
        site_id=selected_site.id,
        is_active=True
    ).all()

    if not urls:
        st.warning("Bu site i√ßin URL bulunamadƒ±!")
        return

    url_options = [f"{u.url_type}: {u.url_path[:50]}..." for u in urls]
    selected_url_idx = st.selectbox("URL Se√ßin", range(len(urls)),
                                   format_func=lambda x: url_options[x])
    selected_url = urls[selected_url_idx]

    # Test butonu
    if st.button("üîç URL'yi Test Et", type="primary"):
        with st.spinner("URL test ediliyor..."):
            test_url = selected_url.url_path
            if not test_url.startswith('http'):
                test_url = selected_site.base_url + test_url

            st.info(f"Test edilen URL: {test_url}")

            # Baƒülantƒ± testi
            try:
                start_time = time.time()
                headers = selected_site.headers if selected_site.headers else {}

                response = requests.get(test_url, headers=headers, timeout=10)
                elapsed_time = time.time() - start_time

                col1, col2, col3 = st.columns(3)

                with col1:
                    if response.status_code == 200:
                        st.success(f"‚úÖ Status: {response.status_code}")
                    else:
                        st.warning(f"‚ö†Ô∏è Status: {response.status_code}")

                with col2:
                    st.metric("Yanƒ±t S√ºresi", f"{elapsed_time:.2f} sn")

                with col3:
                    st.metric("ƒ∞√ßerik Boyutu", f"{len(response.text) / 1024:.1f} KB")

                # HTML analizi
                if response.status_code == 200:
                    st.write("**HTML ƒ∞√ßerik Analizi:**")

                    from bs4 import BeautifulSoup
                    soup = BeautifulSoup(response.text, 'lxml')

                    # √úr√ºn kartlarƒ±nƒ± bul
                    possible_products = []

                    # Yaygƒ±n √ºr√ºn selector'larƒ±
                    common_selectors = [
                        'div[class*="product"]',
                        'div[class*="item"]',
                        'article[class*="product"]',
                        'div[data-testid*="product"]',
                        'div[class*="card"]'
                    ]

                    for selector in common_selectors:
                        elements = soup.select(selector)
                        if elements:
                            possible_products.append((selector, len(elements)))

                    if possible_products:
                        st.write("**Bulunan Muhtemel √úr√ºn Elementleri:**")
                        for selector, count in possible_products:
                            st.write(f"‚Ä¢ `{selector}`: {count} adet")

                    # ƒ∞lk √ºr√ºn√ºn detaylarƒ±nƒ± g√∂ster
                    if possible_products:
                        best_selector = possible_products[0][0]
                        first_product = soup.select_one(best_selector)

                        if first_product:
                            st.write("**ƒ∞lk √úr√ºn HTML √ñrneƒüi:**")
                            # HTML'i g√ºzelle≈ütir ve g√∂ster
                            pretty_html = first_product.prettify()[:1000]
                            st.code(pretty_html, language='html')

                            # Fiyat ve ba≈ülƒ±k tahminleri
                            st.write("**Tespit Edilen Bilgiler:**")

                            # Ba≈ülƒ±k ara
                            title_tags = first_product.find_all(['h1', 'h2', 'h3', 'h4', 'span', 'div'])
                            for tag in title_tags[:5]:
                                text = tag.get_text(strip=True)
                                if len(text) > 10 and len(text) < 200:
                                    st.write(f"Muhtemel ba≈ülƒ±k: `{text[:100]}`")
                                    break

                            # Fiyat ara
                            price_pattern = r'[\d.,]+\s*(TL|‚Ç∫|TRY)'
                            import re
                            prices = re.findall(price_pattern, first_product.get_text())
                            if prices:
                                st.write(f"Muhtemel fiyat: `{prices[0]}`")

            except requests.RequestException as e:
                st.error(f"‚ùå Baƒülantƒ± hatasƒ±: {str(e)}")
            except Exception as e:
                st.error(f"‚ùå Test hatasƒ±: {str(e)}")


def show_site_list():
    """T√ºm siteleri listele"""
    st.subheader("üìä Site Listesi")

    session = get_db_session()

    sites = session.query(SiteConfig).all()

    if not sites:
        st.info("Hen√ºz site eklenmemi≈ü")
        return

    # Site tablosu
    site_data = []
    for site in sites:
        url_count = session.query(SiteUrl).filter_by(site_id=site.id).count()
        active_url_count = session.query(SiteUrl).filter_by(
            site_id=site.id,
            is_active=True
        ).count()

        site_data.append({
            'Site': site.site_name,
            'Kod': site.site_key,
            'URL': site.base_url,
            'Scraper': site.scraper_type,
            'Selenium': '‚úÖ' if site.use_selenium else '‚ùå',
            'URL Sayƒ±sƒ±': f"{active_url_count}/{url_count}",
            'Durum': '‚úÖ Aktif' if site.is_active else '‚ùå Pasif',
            'Rate Limit': f"{site.rate_limit}s"
        })

    df = pd.DataFrame(site_data)
    st.dataframe(df, use_container_width=True)

    # Site silme
    st.markdown("---")
    st.write("**Site Sil:**")

    col1, col2 = st.columns([3, 1])
    with col1:
        site_to_delete = st.selectbox("Silinecek Site",
                                     ["Se√ßin"] + [s.site_name for s in sites])
    with col2:
        if st.button("üóëÔ∏è Siteyi Sil", type="secondary"):
            if site_to_delete != "Se√ßin":
                site = session.query(SiteConfig).filter_by(site_name=site_to_delete).first()
                if site:
                    session.delete(site)
                    session.commit()
                    st.success(f"‚úÖ {site_to_delete} silindi!")
                    st.rerun()


if __name__ == "__main__":
    # Test i√ßin
    st.set_page_config(page_title="Market Spider Admin", page_icon="üõ†Ô∏è", layout="wide")
    show_admin_panel()