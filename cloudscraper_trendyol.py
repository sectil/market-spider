#!/usr/bin/env python3
"""
ðŸ”¥ CLOUDSCRAPER Ä°LE GERÃ‡EK TRENDYOL VERÄ°LERÄ°
Cloudflare bypass iÃ§in cloudscraper kullanÄ±mÄ±
"""

import cloudscraper
import sqlite3
from bs4 import BeautifulSoup
from datetime import datetime
import time
import random
import json

def scrape_with_cloudscraper():
    """Cloudscraper ile Trendyol'dan gerÃ§ek veri Ã§ek"""

    # Cloudscraper instance oluÅŸtur (Cloudflare bypass)
    scraper = cloudscraper.create_scraper(
        browser={
            'browser': 'chrome',
            'platform': 'windows',
            'desktop': True
        }
    )

    conn = sqlite3.connect('market_spider.db')
    cursor = conn.cursor()

    print("="*60)
    print("ðŸ”¥ GERÃ‡EK TRENDYOL VERÄ°LERÄ° Ã‡EKÄ°LÄ°YOR")
    print("="*60)

    # Ã–nce mevcut simÃ¼lasyon verilerini temizle
    print("\nðŸ§¹ SimÃ¼lasyon veriler temizleniyor...")
    cursor.execute("DELETE FROM product_reviews WHERE scraped_at IS NULL")
    cursor.execute("DELETE FROM products WHERE site_name != 'Trendyol'")
    conn.commit()

    try:
        # Ã‡ok satanlar sayfasÄ±
        url = "https://www.trendyol.com/cok-satanlar"
        print(f"\nðŸ“Œ BaÄŸlanÄ±lÄ±yor: {url}")

        response = scraper.get(url)

        if response.status_code == 200:
            print("âœ… Cloudflare bypass baÅŸarÄ±lÄ±!")

            soup = BeautifulSoup(response.text, 'html.parser')

            # Sayfa iÃ§indeki window.__SEARCH_APP_INITIAL_STATE__ deÄŸiÅŸkenini bul
            script_tags = soup.find_all('script')

            for script in script_tags:
                if '__SEARCH_APP_INITIAL_STATE__' in str(script):
                    # JSON verilerini Ã§Ä±kar
                    script_text = str(script.string)
                    start = script_text.find('{')
                    end = script_text.rfind('}') + 1
                    json_str = script_text[start:end]

                    try:
                        data = json.loads(json_str)
                        products = data.get('products', [])

                        print(f"\nðŸ“¦ {len(products)} Ã¼rÃ¼n bulundu!")

                        for idx, product in enumerate(products[:30], 1):
                            try:
                                # ÃœrÃ¼n bilgileri
                                name = product.get('name', '')
                                brand = product.get('brand', {}).get('name', '')
                                price = product.get('price', {}).get('sellingPrice', 0)
                                rating = product.get('ratingScore', {}).get('averageRating', 0)
                                review_count = product.get('ratingScore', {}).get('totalRatingCount', 0)
                                product_id = product.get('id')
                                merchant_id = product.get('merchant', {}).get('id')
                                url = f"https://www.trendyol.com{product.get('url', '')}"

                                # VeritabanÄ±na ekle
                                cursor.execute("""
                                    INSERT OR REPLACE INTO products (
                                        name, brand, price, rating, review_count,
                                        url, site_name, in_stock, created_at
                                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                                """, (
                                    name, brand, price, rating, review_count,
                                    url, 'Trendyol', 1, datetime.now()
                                ))

                                db_product_id = cursor.lastrowid
                                print(f"  {idx}. âœ… {name[:40]}")

                                # YorumlarÄ± Ã§ek
                                if product_id and merchant_id:
                                    time.sleep(random.uniform(1, 2))

                                    review_url = f"https://public-mdc.trendyol.com/discovery-web-socialgw-service/api/rating/review?merchantId={merchant_id}&productId={product_id}&page=0&size=20"

                                    review_response = scraper.get(review_url)

                                    if review_response.status_code == 200:
                                        review_data = review_response.json()
                                        reviews = review_data.get('result', {}).get('productReviews', {}).get('content', [])

                                        for review in reviews[:10]:
                                            reviewer = review.get('userFullName', 'Anonim')
                                            rating = review.get('rate', 0)
                                            comment = review.get('comment', '')
                                            review_date = review.get('lastModifiedDate')

                                            if review_date:
                                                review_date = datetime.fromtimestamp(review_date / 1000)
                                            else:
                                                review_date = datetime.now()

                                            cursor.execute("""
                                                INSERT INTO product_reviews (
                                                    product_id, reviewer_name, rating,
                                                    review_text, review_date, scraped_at,
                                                    sentiment_score, helpful_count
                                                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                                            """, (
                                                db_product_id, reviewer, rating, comment,
                                                review_date, datetime.now(),
                                                1.0 if rating >= 4 else -1.0 if rating <= 2 else 0.0,
                                                review.get('helpfulCount', 0)
                                            ))

                                        print(f"     ðŸ’¬ {len(reviews)} yorum eklendi")

                                conn.commit()

                            except Exception as e:
                                print(f"  âŒ ÃœrÃ¼n hatasÄ±: {e}")
                                continue

                    except json.JSONDecodeError as e:
                        print(f"JSON parse hatasÄ±: {e}")

            # Alternatif: HTML parse et
            if not products:
                print("\nðŸ“‹ HTML parse deneniyor...")

                product_cards = soup.find_all('div', class_='p-card-wrppr')[:20]

                for idx, card in enumerate(product_cards, 1):
                    try:
                        name_elem = card.find('span', class_='prdct-desc-cntnr-name')
                        brand_elem = card.find('span', class_='prdct-desc-cntnr-ttl')
                        price_elem = card.find('div', class_='prc-box-dscntd')
                        link_elem = card.find('a')

                        if name_elem and price_elem:
                            name = name_elem.text.strip()
                            brand = brand_elem.text.strip() if brand_elem else 'Bilinmiyor'
                            price = float(price_elem.text.replace(' TL', '').replace(',', '.'))
                            url = 'https://www.trendyol.com' + link_elem['href'] if link_elem else ''

                            cursor.execute("""
                                INSERT INTO products (
                                    name, brand, price, url, site_name,
                                    in_stock, created_at
                                ) VALUES (?, ?, ?, ?, ?, ?, ?)
                            """, (name, brand, price, url, 'Trendyol', 1, datetime.now()))

                            print(f"  {idx}. âœ… {name[:40]}")

                    except Exception as e:
                        continue

                conn.commit()

        else:
            print(f"âŒ HTTP {response.status_code} hatasÄ±")

    except Exception as e:
        print(f"âŒ BaÄŸlantÄ± hatasÄ±: {e}")

    finally:
        # SonuÃ§ raporu
        cursor.execute("SELECT COUNT(*) FROM products WHERE site_name='Trendyol'")
        product_count = cursor.fetchone()[0]

        cursor.execute("SELECT COUNT(*) FROM product_reviews WHERE scraped_at IS NOT NULL")
        review_count = cursor.fetchone()[0]

        print("\n" + "="*60)
        print("ðŸ“Š SONUÃ‡")
        print("="*60)
        print(f"âœ… Toplam Ã¼rÃ¼n: {product_count}")
        print(f"ðŸ’¬ Toplam yorum: {review_count}")
        print("="*60)

        conn.close()

if __name__ == "__main__":
    # Cloudscraper kurulumu kontrol et
    try:
        import cloudscraper
        print("âœ… Cloudscraper kurulu")
    except ImportError:
        print("ðŸ“¦ Cloudscraper kuruluyor...")
        import subprocess
        subprocess.run(["pip", "install", "cloudscraper"], check=True)
        import cloudscraper

    scrape_with_cloudscraper()